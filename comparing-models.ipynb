{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import cupy as cp\n",
    "from cupy.random import seed\n",
    "seed(42)\n",
    "import plotly.express as px\n",
    "from DBN.dbn import SupervisedDBNClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Functions\n",
    "def pklSave(contentToBeSaved, fullPath):\n",
    "    with open(fullPath, 'wb') as f:\n",
    "        pickle.dump(contentToBeSaved, f)\n",
    "\n",
    "def pklLoad(fullPath, convertToNumpyArray=False):\n",
    "    with open(fullPath, 'rb') as f:\n",
    "        content = pickle.load(f)\n",
    "    if convertToNumpyArray:\n",
    "        content = np.array(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_learning_curve(estimator, x, y, train_sizes = None, test_sizes = None): # pass x, y as lists or nparrays\n",
    "    if train_sizes is None:\n",
    "        train_sizes = [0.2,0.4,0.6,0.8,1]\n",
    "    if test_sizes is None:\n",
    "        test_sizes = [0.2,0.2,0.2,0.2,0.2]\n",
    "    \n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    if type(estimator) == SupervisedDBNClassification:\n",
    "        for i in range(len(train_sizes)):\n",
    "            if train_sizes[i] == 1:\n",
    "                x_new, y_new = x, y\n",
    "            else:\n",
    "                x_new, _, y_new, _ = train_test_split(x, y, test_size=1-train_sizes[i], random_state=42)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=test_sizes[i], random_state=42)\n",
    "            x_train = cp.array(x_train)\n",
    "            x_test = cp.array(x_test)\n",
    "            y_train = cp.array(y_train) \n",
    "            y_test = cp.array(y_test)\n",
    "\n",
    "            estimator.fit(x_train, y_train)\n",
    "\n",
    "            y_pred_train = np.array(estimator.predict(x_train))\n",
    "            y_pred_test = np.array(estimator.predict(x_test))\n",
    "            y_train = np.array(y_train.get()) #converts cp array to np array to be compatible with accuracy_score\n",
    "            y_test = np.array(y_test.get())\n",
    "\n",
    "            train_accs.append(accuracy_score(y_train, y_pred_train))\n",
    "            test_accs.append(accuracy_score(y_test, y_pred_test))\n",
    "        return (train_sizes, train_accs, test_accs)\n",
    "\n",
    "\n",
    "    if type(estimator) == \"WRITE CLASS NAME OF MLP HERE PLZ\":\n",
    "        for i in range(len(train_sizes)):\n",
    "            if train_sizes[i] == 1:\n",
    "                x_new, y_new = x, y\n",
    "            else:\n",
    "                x_new, _, y_new, _ = train_test_split(x, y, test_size=1-train_sizes[i], random_state=42)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=test_sizes[i], random_state=42)\n",
    "            x_train = np.array(x_train)\n",
    "            x_test = np.array(x_test)\n",
    "            y_train = np.array(y_train) \n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            estimator.fit(x_train, y_train)\n",
    "\n",
    "            y_pred_train = np.array(estimator.predict(x_train))\n",
    "            y_pred_test = np.array(estimator.predict(x_test))\n",
    "\n",
    "            train_accs.append(accuracy_score(y_train, y_pred_train))\n",
    "            test_accs.append(accuracy_score(y_test, y_pred_test))\n",
    "        return (train_sizes, train_accs, test_accs)\n",
    "\n",
    "    if type(estimator) == \"WRITE CLASS NAME OF HF HERE PLZ\":\n",
    "        for i in range(len(train_sizes)):\n",
    "            if train_sizes[i] == 1:\n",
    "                x_new, y_new = x, y\n",
    "            else:\n",
    "                x_new, _, y_new, _ = train_test_split(x, y, test_size=1-train_sizes[i], random_state=42)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=test_sizes[i], random_state=42)\n",
    "            x_train = np.array(x_train)\n",
    "            x_test = np.array(x_test)\n",
    "            y_train = np.array(y_train) \n",
    "            y_test = np.array(y_test)\n",
    "\n",
    "            estimator.fit(x_train, y_train)\n",
    "\n",
    "            y_pred_train = np.array(estimator.predict(x_train))\n",
    "            y_pred_test = np.array(estimator.predict(x_test))\n",
    "\n",
    "            train_accs.append(accuracy_score(y_train, y_pred_train))\n",
    "            test_accs.append(accuracy_score(y_test, y_pred_test))\n",
    "        return (train_sizes, train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annsTrAccs = []\n",
    "annsTsAccs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn = pklLoad(\"DBN/finalDbn.pickle\")\n",
    "dbnTrAcc, dbnTsAcc = pklLoad(\"DBN/finalDbnTrTsAccs.pickle\")\n",
    "annsTrAccs.append(dbnTrAcc)\n",
    "annsTsAccs.append(dbnTsAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = pklLoad(\"MLP/finalMlp.pickle\")\n",
    "mlpTrAcc, mlpTsAcc = pklLoad(\"MLP/finalMlpTrTsAccs.pickle\")\n",
    "annsTrAccs.append(mlpTrAcc)\n",
    "annsTsAccs.append(mlpTsAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = pklLoad(\"HF/finalHf.pickle\")\n",
    "hfTrAcc, hfTsAcc = pklLoad(\"HF/finalHfTrTsAccs.pickle\")\n",
    "annsTrAccs.append(hfTrAcc)\n",
    "annsTsAccs.append(hfTsAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annNames = [\n",
    "    \"DBN\", \n",
    "    \"MLP\",\n",
    "    \"HF\"\n",
    "]\n",
    "pklSave((annNames, annsTrAccs, annsTsAccs), \"annsAccs.pickle\")\n",
    "dfsNames, annsTrAccs, annsTsAccs = pklLoad(\"annsAccs.pickle\")\n",
    "dfsNames, annsTrAccs, annsTsAccs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between Each Model Using Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x128aug = pklLoad('../x128aug.pkl', convertToNumpyArray=True)\n",
    "y = pklLoad('../yaug.pkl', convertToNumpyArray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbn_train_sizes, dbn_train_accs, dbn_test_accs = custom_learning_curve(dbn, x128aug, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from ML labs (or preferrably, plotly) to draw learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_train_sizes, mlp_train_accs, mlp_test_accs = custom_learning_curve(mlp, x128aug, y) # change x128aug based to the dataset from which you got the best scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from ML labs (or preferrably, plotly) to draw learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train_sizes, hf_train_accs, hf_test_accs = custom_learning_curve(hf, x128aug, y) # change x128aug based to the dataset from which you got the best scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from ML labs (or preferrably, plotly) to draw learning curves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Comment on which model generalizes best (or if they all don't generalize well), while mentioning how you deduced so from the plotted learning curves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between Each Model Using Final Accuracies Obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_legend_name(fig, new_names):\n",
    "    for i, new_name in enumerate(new_names):\n",
    "        fig.data[i].name = new_name\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=annNames, y=[annsTrAccs, annsTsAccs], markers=True)\n",
    "fig.update_layout(title=\"Comparing Different ANN Models on Wonders of The World Dataset\", legend_title=\"Accuracy Type\")\n",
    "fig.update_layout(xaxis={\"title\": \"ANN Model\"})\n",
    "fig = custom_legend_name(fig, [\"Training Accuracy\", \"Testing Accuracy\"])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Comment on which model has the best final accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a820c38c0c6998d5531b30a72abed1c849220b8ccc9e18c2545958238bc5c1c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
