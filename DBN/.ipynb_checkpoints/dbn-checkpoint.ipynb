{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Reproducable Code\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)\n",
    "from cupy.random import seed\n",
    "seed(42)\n",
    "\n",
    "# other standard imports. \"pip install -r requirements.txt\" to install dependencies and \"pip freeze > requirements.txt\" to update them\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import PIL\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dbn import SupervisedDBNClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Functions\n",
    "def pklSave(contentToBeSaved, fullPath):\n",
    "    with open(fullPath, 'wb') as f:\n",
    "        pickle.dump(contentToBeSaved, f)\n",
    "\n",
    "def pklLoad(fullPath, convertToNumpyArray=False):\n",
    "    with open(fullPath, 'rb') as f:\n",
    "        content = pickle.load(f)\n",
    "    if convertToNumpyArray:\n",
    "        content = np.array(content)\n",
    "    return content\n",
    "\n",
    "def train_val_test_split(x, y, tr, va, te, **kwargs):\n",
    "    x_train, x_test_val, y_train, y_test_val = train_test_split(x, y, test_size=1-tr)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test_val, y_test_val, test_size=te/(te+va), **kwargs)\n",
    "    return (x_train, x_val, x_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_learning_curve(estimator, x, y, train_sizes = None, test_sizes = None): # pass x, y as lists or nparrays\n",
    "    if train_sizes is None:\n",
    "        train_sizes = [0.2,0.4,0.6,0.8,1]\n",
    "    if test_sizes is None:\n",
    "        test_sizes = [0.2,0.2,0.2,0.2,0.2]\n",
    "    \n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    if type(estimator) == SupervisedDBNClassification:\n",
    "        for i in range(len(train_sizes)):\n",
    "            if train_sizes[i] == 1:\n",
    "                x_new, y_new = x, y\n",
    "            else:\n",
    "                x_new, _, y_new, _ = train_test_split(x, y, test_size=1-train_sizes[i], random_state=42)\n",
    "\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=test_sizes[i], random_state=42)\n",
    "            x_train = cp.array(x_train)\n",
    "            x_test = cp.array(x_test)\n",
    "            y_train = cp.array(y_train) \n",
    "            y_test = cp.array(y_test)\n",
    "\n",
    "            estimator.fit(x_train, y_train)\n",
    "\n",
    "            y_pred_train = np.array(estimator.predict(x_train))\n",
    "            y_pred_test = np.array(estimator.predict(x_test))\n",
    "            y_train = np.array(y_train.get()) #converts cp array to np array to be compatible with accuracy_score\n",
    "            y_test = np.array(y_test.get())\n",
    "\n",
    "            train_accs.append(accuracy_score(y_train, y_pred_train))\n",
    "            test_accs.append(accuracy_score(y_test, y_pred_test))\n",
    "        return (train_sizes, train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsTrAccs = [] # Tr == Training\n",
    "dfsTsAccs = [] # Ts == Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model on Rescaled Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pklLoad('../x128.pkl', convertToNumpyArray=True)\n",
    "y = pklLoad('../y.pkl', convertToNumpyArray=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "x_train = cp.array(x_train)\n",
    "x_test = cp.array(x_test)\n",
    "y_train = cp.array(y_train)\n",
    "y_test = cp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436, 360, 1436, 360)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436, 16384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128],\n",
    "                                         learning_rate_rbm=0.1, #0.05 with 50 epochs == 0.1 with 20 epochs\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5, # loss was found to be stagnating after this value\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1) # low drop-out value, as model is underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Results:\n",
    "<br><br>\n",
    "<img src=\"notebook_media/dbn128training.png\" height=800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.save('dbnModel128.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification.load('dbnModel128.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDbnResults(classifier, trAccs=None, tsAccs=None):\n",
    "    \"\"\"Note: assumes you globally defined x_train/test & y_train/test before calling the function\"\"\"\n",
    "    y_pred = classifier.predict(x_train) # predict returns a list\n",
    "    y_pred = np.array(y_pred) # converting to nparray to be compatible with accuracy_score\n",
    "    y_train_np = np.array(y_train.get())\n",
    "    y_test_np = np.array(y_test.get())\n",
    "    dbn_train_score = accuracy_score(y_train_np, y_pred)\n",
    "    print(f'Training Accuracy: {dbn_train_score}')\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    dbn_test_score = accuracy_score(y_test_np, y_pred)\n",
    "    print(f'Testing Accuracy: {dbn_test_score}')\n",
    "\n",
    "    trAccs.append(dbn_train_score)\n",
    "    tsAccs.append(dbn_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.387883008356546\n",
      "Testing Accuracy: 0.33055555555555555\n"
     ]
    }
   ],
   "source": [
    "printDbnResults(classifier, dfsTrAccs, dfsTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model on Features Extracted From the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = pklLoad('../x128f.pkl', convertToNumpyArray=True)\n",
    "y = pklLoad('../y.pkl', convertToNumpyArray=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(xf, y, test_size=0.2, random_state=42)\n",
    "x_train = cp.array(x_train)\n",
    "x_test = cp.array(x_test)\n",
    "y_train = cp.array(y_train)\n",
    "y_test = cp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128],\n",
    "                                         learning_rate_rbm=0.1,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Results: <br>\n",
    "<img src='notebook_media/dbn128ftraining.png' height=850 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.save('dbnModel128f.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification.load('dbnModel128f.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.19707520891364902\n",
      "Testing Accuracy: 0.21388888888888888\n"
     ]
    }
   ],
   "source": [
    "printDbnResults(classifier, dfsTrAccs, dfsTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = pklLoad('../x64f.pkl', convertToNumpyArray=True)\n",
    "y = pklLoad('../y.pkl', convertToNumpyArray=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(xf, y, test_size=0.2, random_state=42)\n",
    "x_train = cp.array(x_train)\n",
    "x_test = cp.array(x_test)\n",
    "y_train = cp.array(y_train)\n",
    "y_test = cp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128],\n",
    "                                         learning_rate_rbm=0.1,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Results: <br>\n",
    "<img src='notebook_media/dbn64ftraining.png' height=850 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.save('dbnModel64f.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification.load('dbnModel64f.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.19498607242339833\n",
      "Testing Accuracy: 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "printDbnResults(classifier, dfsTrAccs, dfsTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model on Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x128aug = pklLoad('../x128aug.pkl', convertToNumpyArray=True)\n",
    "y = pklLoad('../yaug.pkl', convertToNumpyArray=True)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x128aug, y, test_size=0.2, random_state=42)\n",
    "x_train = cp.array(x_train)\n",
    "x_test = cp.array(x_test)\n",
    "y_train = cp.array(y_train)\n",
    "y_test = cp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128],\n",
    "                                         learning_rate_rbm=0.1,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Result: <br>\n",
    "<img src='notebook_media/dbn128augtraining.png' height=850 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.save('dbnModel128aug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification.load('dbnModel128aug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4668522707499565\n",
      "Testing Accuracy: 0.46903270702853167\n"
     ]
    }
   ],
   "source": [
    "printDbnResults(classifier, dfsTrAccs, dfsTsAccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['128x128',\n",
       "  '1.5K PCA Extracted Features from 128x128',\n",
       "  '1.25K PCA Extracted Features from 64x64',\n",
       "  '128x128 Augmented (4-Folds)'],\n",
       " [0.387883008356546,\n",
       "  0.19707520891364902,\n",
       "  0.19498607242339833,\n",
       "  0.4668522707499565],\n",
       " [0.33055555555555555,\n",
       "  0.21388888888888888,\n",
       "  0.2222222222222222,\n",
       "  0.46903270702853167])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfsNames = [\"128x128\", \"1.5K PCA Extracted Features from 128x128\", \"1.25K PCA Extracted Features from 64x64\", \"128x128 Augmented (4-Folds)\"]\n",
    "# pklSave((dfsNames, dfsTrAccs, dfsTsAccs), \"dfsDbnAccs.pickle\")\n",
    "dfsNames, dfsTrAccs, dfsTsAccs = pklLoad(\"dfsDbnAccs.pickle\")\n",
    "dfsNames, dfsTrAccs, dfsTsAccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_legend_name(fig, new_names):\n",
    "    for i, new_name in enumerate(new_names):\n",
    "        fig.data[i].name = new_name\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mline(x\u001b[39m=\u001b[39mdfsNames, y\u001b[39m=\u001b[39m[dfsTrAccs, dfsTsAccs], markers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mComparing DBN Performance on Different Pre-Processed Datasets\u001b[39m\u001b[39m\"\u001b[39m, legend_title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy Type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(xaxis\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mAlteration Done on Images\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'px' is not defined"
     ]
    }
   ],
   "source": [
    "fig = px.line(x=dfsNames, y=[dfsTrAccs, dfsTsAccs], markers=True)\n",
    "fig.update_layout(title=\"Comparing DBN Performance on Different Pre-Processed Datasets\", legend_title=\"Accuracy Type\")\n",
    "fig.update_layout(xaxis={\"title\": \"Alteration Done on Images\"})\n",
    "fig = custom_legend_name(fig, [\"Training Accuracy\", \"Testing Accuracy\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning on 128-Aug. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the augmented dataset on the rescaled 128x128 images yielded the best results, given that all DBN models had the same hyper-parameters. <br>\n",
    "Therefore, let's further train the last DBN model on the augmented dataset with different hyper-parameters.\n",
    "<br>\n",
    "Recall that this was our initial 128AugDBN Hyper-Parameters: \n",
    "<br><br>\n",
    "<img src=\"notebook_media/dbn128augparams.png\" width=300 />\n",
    "<br><br>\n",
    "with this fitting result:\n",
    "<br><br>\n",
    "<img src=\"notebook_media/dbn128augtraining.png\" height=850 />\n",
    "<br><br>\n",
    "Which gave out these accuracies: <br>\n",
    "0.4668522707499565 Training Accuracy <br>\n",
    "0.46903270702853167 Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df128augTrAccs = [dfsTrAccs[-1]]\n",
    "df128augTsAccs = [dfsTsAccs[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing RBM's Learning Rate\n",
    "It makes sense that this is the first hyper-parameter to tune, since previously, we saw high RBM reconstruction errors that slowly converged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128],\n",
    "                                         learning_rate_rbm=0.15,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.464068209500609\n",
      "Testing Accuracy: 0.4349338900487126\n"
     ]
    }
   ],
   "source": [
    "# classifier.fit(x_train, y_train)\n",
    "# classifier.save('dbnModel128augRbmLR.pkl')\n",
    "classifier = SupervisedDBNClassification.load('dbnModel128augRbmLR.pkl')\n",
    "printDbnResults(classifier, df128augTrAccs, df128augTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Result: <br>\n",
    "<img src='notebook_media/dbn128augRbmLRtraining.png' height=850 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing NN's Learning Rate\n",
    "The training loss at the fine-tuning (back-propagation) step was also slowly converging, so let's increase its learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128],\n",
    "                                         learning_rate_rbm=0.1,\n",
    "                                         learning_rate=0.15,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.47711849660692535\n",
      "Testing Accuracy: 0.4516353514265832\n"
     ]
    }
   ],
   "source": [
    "# classifier.fit(x_train, y_train)\n",
    "# classifier.save('dbnModel128augNNLR.pkl')\n",
    "classifier = SupervisedDBNClassification.load('dbnModel128augNNLR.pkl')\n",
    "printDbnResults(classifier, df128augTrAccs, df128augTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Result: <br>\n",
    "<img src='notebook_media/dbn128augNNLRtraining.png' height=850 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing #Hidden-Layers\n",
    "This alteration could negligibly affect the model's performance, as we saw very low reconstruction errors in the second RBM, so adding a third one won't probably be that beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128, 128],\n",
    "                                         learning_rate_rbm=0.1,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4764224812945885\n",
      "Testing Accuracy: 0.4704244954766875\n"
     ]
    }
   ],
   "source": [
    "# classifier.fit(x_train, y_train)\n",
    "# classifier.save('dbnModel128aug3L.pkl')\n",
    "classifier = SupervisedDBNClassification.load('dbnModel128aug3L.pkl')\n",
    "printDbnResults(classifier, df128augTrAccs, df128augTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Result: <br>\n",
    "<img src='notebook_media/dbn128aug3Ltraining.png' height=850 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing #Hidden-Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
    "                                         learning_rate_rbm=0.1,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=20,\n",
    "                                         n_iter_backprop=5,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4082129806855751\n",
      "Testing Accuracy: 0.4036186499652053\n"
     ]
    }
   ],
   "source": [
    "# classifier.fit(x_train, y_train)\n",
    "# classifier.save('dbnModel128aug256HN.pkl')\n",
    "classifier = SupervisedDBNClassification.load('dbnModel128aug256HN.pkl')\n",
    "printDbnResults(classifier, df128augTrAccs, df128augTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting Result: <br>\n",
    "<img src='notebook_media/dbn128aug256HNtraining.png' height=850 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing RBM & Back Propagation's #Epochs \n",
    "We'll also decrease the RBM's learning by half, since we increased RBM's #Epocs by the double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[128, 128],\n",
    "                                         learning_rate_rbm=0.05,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=60,\n",
    "                                         n_iter_backprop=10,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='sigmoid',\n",
    "                                         dropout_p=0.1,\n",
    "                                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4939968679310945\n",
      "Testing Accuracy: 0.46555323590814196\n"
     ]
    }
   ],
   "source": [
    "# classifier.fit(x_train, y_train)\n",
    "# classifier.save('dbnModel128augEpochsAndRbmLR.pkl')\n",
    "classifier = SupervisedDBNClassification.load('dbnModel128augEpochsAndRbmLR.pkl')\n",
    "printDbnResults(classifier, df128augTrAccs, df128augTsAccs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large output, no screenshot of the errors is provided. <br>\n",
    "Either way, the final RBM & NN errors were similar to the original DBN used on 128-Aug dataset <br>**except** the first layer's RBM reconstruction error which converged to approximately `500` instead of `900`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing The Hyper-Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['HL:[128HN, 128HN], RBM/NN LR: 0.1, RBM #Epochs: 20, BP #Epochs: 5',\n",
       "  'RBM LR: 0.3',\n",
       "  'NN LR: 0.3',\n",
       "  'HL: [128,128,128]',\n",
       "  'HL: [256,256]',\n",
       "  'RBM #Epochs: 100, NN #Epochs: 20, RBM LR: 0.05'],\n",
       " [0.4668522707499565,\n",
       "  0.464068209500609,\n",
       "  0.47711849660692535,\n",
       "  0.4764224812945885,\n",
       "  0.4082129806855751,\n",
       "  0.4939968679310945],\n",
       " [0.46903270702853167,\n",
       "  0.4349338900487126,\n",
       "  0.4516353514265832,\n",
       "  0.4704244954766875,\n",
       "  0.4036186499652053,\n",
       "  0.46555323590814196])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htDbnsNames = [\n",
    "    \"HL:[128HN, 128HN], RBM/NN LR: 0.1, RBM #Epochs: 20, BP #Epochs: 5\", \n",
    "    \"RBM LR: 0.3\", \n",
    "    \"NN LR: 0.3\", \n",
    "    \"HL: [128,128,128]\", \n",
    "    \"HL: [256,256]\", \n",
    "    \"RBM #Epochs: 100, NN #Epochs: 20, RBM LR: 0.05\"\n",
    "]\n",
    "pklSave((htDbnsNames, df128augTrAccs, df128augTsAccs), \"df128augAccs.pickle\")\n",
    "dfsNames, df128augTrAccs, df128augTsAccs = pklLoad(\"df128augAccs.pickle\")\n",
    "dfsNames, df128augTrAccs, df128augTsAccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig \u001b[39m=\u001b[39m px\u001b[39m.\u001b[39mline(x\u001b[39m=\u001b[39mdfsNames, y\u001b[39m=\u001b[39m[df128augTrAccs, df128augTsAccs], markers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mComparing Different DBNs on 128x128 4-Folds Augmented Dataset\u001b[39m\u001b[39m\"\u001b[39m, legend_title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy Type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m fig\u001b[39m.\u001b[39mupdate_layout(xaxis\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mAlteration Done on DBN\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms Hyperparameters (1st one is the original)\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'px' is not defined"
     ]
    }
   ],
   "source": [
    "fig = px.line(x=dfsNames, y=[df128augTrAccs, df128augTsAccs], markers=True)\n",
    "fig.update_layout(title=\"Comparing Different DBNs on 128x128 4-Folds Augmented Dataset\", legend_title=\"Accuracy Type\")\n",
    "fig.update_layout(xaxis={\"title\": \"Alteration Done on DBN's Hyperparameters (1st one is the original)\"})\n",
    "fig = custom_legend_name(fig, [\"Training Accuracy\", \"Testing Accuracy\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the performance increased when we <font color=\"red\"> increased blah blah blah </font>\n",
    "<br>\n",
    "Therefore, we'll save this model along with its results as the final DBN model to be trained and compared with the other ANN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SupervisedDBNClassification.load('dbnModel128augEpochsAndRbmLR.pkl')\n",
    "pklSave(classifier, \"finalDbn.pickle\")\n",
    "pklSave((df128augTrAccs[-1], df128augTsAccs[-1]), \"finalDbnTrTsAccs.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbn = pklLoad(\"DBN/finalDbn.pickle\")\n",
    "# dbnTrAcc, dbnTsAcc = pklLoad(\"DBN/finalDbnTrTsAccs.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a820c38c0c6998d5531b30a72abed1c849220b8ccc9e18c2545958238bc5c1c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
